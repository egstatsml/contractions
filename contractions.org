#+PROPERTY: header-args:jupyter-python  :session  /home/ethan/.local/share/jupyter/runtime/kernel-f129ff30-10e2-462a-bfbe-dcc5a97bdab5.json
#+title: Contractions

I don't like typing apostrophes. Making my pinky do extra work  and stretching far all the time makes me sad.   This notebook is an attempt to alleviate
that sadness.

I live inside Emacs, so have the flexibility to incorporate some my own logic to address my woes. The current plan of attack is;
1. Find list of most common used words
   1. Find top "n" words with apostrophes
      1. Put them into an abbrev table with the appostrophes removed, and let Emacs and abbrev do the heavy lifting for me.

** Word list
I am going  to use the word list from [[https://github.com/IlyaSemenov/wikipedia-word-frequency][Wikipedia Word Frequency]] to help me find words with apostrophes. This package also has an order to let us know how common
the words are, so I can say just get the top 1000 words with apostrophes.

I have made this repo a submodule, which includes the list  [[file:wikipedia-word-frequency/results/enwiki-20210820-words-frequency.txt][from 2021 analysis here]].
I am going to filter out to just get the samples with  apostrophes.

#+begin_src bash
  grep \' wikipedia-word-frequency/results/enwiki-20210820-words-frequency.txt >> apostrophe_list.txt
#+end_src

#+RESULTS:

This will result in a file with the format of
#+begin_src
<word> <frequency>
#+end_src
which is essentially a csv file except with spaces, and is ordered so that the most frequent words will be at the top.

I am going to use python to process this, as am more comfortable with that.
    #+begin_src python :results raw
      import pandas as pd

      def remove_apostrophe(row):
          """act on row of dataframe to remove apostrophes.

          Words with apostrophes removed will be placed in the "removed" column.

          Parameters
          ----------
          row : pd.Series
              Row from the dataframe of interest.

          Examples
          --------
          FIXME: Add docs.
          """
          return row.word.replace("\'", "")

      print("here")
      # load in the file with apostrophes
      wikipedia_df = pd.read_csv("apostrophe_list.txt", sep=" ", names=["word", "frequency"])
      # let's add an empty column to contain the words without apostrophes.
      wikipedia_df["removed"] = wikipedia_df.apply(remove_apostrophe, axis=1)

      print(wikipedia_df.head())
      print(wikipedia_df.shape)
    #+end_src

    #+RESULTS:
    None

    #+begin_src jupyter-python
      import pandas as pd
      import os

      def remove_apostrophe(row):
          """act on row of dataframe to remove apostrophes.

          Words with apostrophes removed will be placed in the "removed" column.

          Parameters
          ----------
          row : pd.Series
              Row from the dataframe of interest.

          Examples
          --------
          FIXME: Add docs.
          """
          return row.word.replace("\'", "")

      print("here")
      os.chdir("/home/ethan/code/contractions")
      print(os.getcwd())
      # load in the file with apostrophes
      wikipedia_df = pd.read_csv("./apostrophe_list.txt", sep=" ", names=["word", "frequency"])
      # let's add an empty column to contain the words without apostrophes.
      wikipedia_df["removed"] = wikipedia_df.apply(remove_apostrophe, axis=1)
      print(wikipedia_df.head())
      print(wikipedia_df.shape)
    #+end_src

    #+RESULTS:
    : here
    : /home/ethan/code/contractions
    :          word  frequency    removed
    : 0     women's     503960     womens
    : 1       men's     354399       mens
    : 2        it's     228179        its
    : 3    people's     152424    peoples
    : 4  children's     137591  childrens
    : (173030, 3)
    This results in a pretty big list of words with apostrophes. Lets have a look at some them to see what they are like.
    #+begin_src jupyter-python
      import numpy as np
      # let's have a look at say every thousandth word.
      idx = np.linspace(1000, wikipedia_df.shape[0], 100).astype(np.int64)
      idx = np.linspace(0, 100, 100).astype(np.int64)
      for i in idx:
          print(f"idx: {i}, word: {wikipedia_df.word.iloc[i]}")

    #+end_src

    #+RESULTS:
    #+begin_example
      idx: 0, word: women's
      idx: 1, word: men's
      idx: 2, word: it's
      idx: 3, word: people's
      idx: 4, word: children's
      idx: 5, word: don't
      idx: 6, word: band's
      idx: 7, word: city's
      idx: 8, word: world's
      idx: 9, word: didn't
      idx: 10, word: company's
      idx: 11, word: father's
      idx: 12, word: king's
      idx: 13, word: i'm
      idx: 14, word: club's
      idx: 15, word: team's
      idx: 16, word: country's
      idx: 17, word: album's
      idx: 18, word: school's
      idx: 19, word: master's
      idx: 20, word: film's
      idx: 21, word: doesn't
      idx: 22, word: that's
      idx: 23, word: group's
      idx: 24, word: state's
      idx: 25, word: year's
      idx: 26, word: can't
      idx: 27, word: party's
      idx: 28, word: he's
      idx: 29, word: university's
      idx: 30, word: queen's
      idx: 31, word: government's
      idx: 32, word: wasn't
      idx: 33, word: bachelor's
      idx: 34, word: john's
      idx: 35, word: mary's
      idx: 36, word: one's
      idx: 37, word: show's
      idx: 38, word: mother's
      idx: 39, word: game's
      idx: 40, word: town's
      idx: 41, word: there's
      idx: 42, word: nation's
      idx: 43, word: america's
      idx: 44, word: family's
      idx: 45, word: you're
      idx: 46, word: australia's
      idx: 47, word: today's
      idx: 48, word: i've
      idx: 49, word: station's
      idx: 50, word: canada's
      idx: 51, word: league's
      idx: 52, word: she's
      idx: 53, word: o'brien
      idx: 54, word: song's
      idx: 55, word: man's
      idx: 56, word: isn't
      idx: 57, word: we're
      idx: 58, word: couldn't
      idx: 59, word: ship's
      idx: 60, word: india's
      idx: 61, word: paul's
      idx: 62, word: china's
      idx: 63, word: woman's
      idx: 64, word: army's
      idx: 65, word: earth's
      idx: 66, word: george's
      idx: 67, word: god's
      idx: 68, word: president's
      idx: 69, word: peter's
      idx: 70, word: they're
      idx: 71, word: smith's
      idx: 72, word: season's
      idx: 73, word: district's
      idx: 74, word: london's
      idx: 75, word: magazine's
      idx: 76, word: church's
      idx: 77, word: britain's
      idx: 78, word: germany's
      idx: 79, word: o'connor
      idx: 80, word: japan's
      idx: 81, word: husband's
      idx: 82, word: won't
      idx: 83, word: player's
      idx: 84, word: society's
      idx: 85, word: what's
      idx: 86, word: person's
      idx: 87, word: county's
      idx: 88, word: let's
      idx: 89, word: o'neill
      idx: 90, word: wouldn't
      idx: 91, word: york's
      idx: 92, word: building's
      idx: 93, word: character's
      idx: 94, word: island's
      idx: 95, word: i'll
      idx: 96, word: court's
      idx: 97, word: latter's
      idx: 98, word: organization's
      idx: 100, word: who's
    #+end_example

    Is some useful words here, but it looks like it might be missing a lot of words relevent for first person writing, which I still need. To try and alleviate this,  am going to include some of the words from the [[http://wordlist.aspell.net/scowl-readme/][SCOWL]] word lists, particularly the list of contractions.

   First lets load in all the contraction words and drop any duplicates.

   #+begin_src jupyter-python
     data_dir = "/home/ethan/data/scowl-2020.12.07"
     # get names of all the contraction files
     import glob
     contraction_files = glob.glob(data_dir + "/final/*contraction*")
     scowl_file = os.path.join("/home/ethan/code/contractions/scowl_contractions_combined.txt")
     with open(scowl_file, "w") as outfile:
         for file_path in contraction_files:
             with open(file_path, "r") as infile:
                 for line in infile:
                     outfile.write(line)
   #+end_src

   #+RESULTS:

   Let's now combine these lists.
   I want to first trim down the original list, maybe keep the first 1000 most common. Will then combine and them remove any duplicates.

   #+begin_src jupyter-python
     appostrophe_df = wikipedia_df.iloc[0:1000, :]
     # now lets load in the scowl contractions
     scowl_df = pd.read_csv(scowl_file, names=["words"])
     scowl_df['removed'] = 0
     scowl_df['frequency'] = 0
     scowl_df.head()
   #+end_src

   #+RESULTS:
   :RESULTS:
   # [goto error]
   : [0;31m---------------------------------------------------------------------------[0m
   : [0;31mNameError[0m                                 Traceback (most recent call last)
   : Input [0;32mIn [16][0m, in [0;36m<cell line: 1>[0;34m()[0m
   : [0;32m----> 1[0m appostrophe_df [38;5;241m=[39m [43mwikipedia_df[49m[38;5;241m.[39miloc[[38;5;241m0[39m:[38;5;241m1000[39m, :]
   : [1;32m      2[0m [38;5;66;03m# now lets load in the scowl contractions [39;00m
   : [1;32m      3[0m scowl_df [38;5;241m=[39m pd[38;5;241m.[39mread_csv(scowl_file, names[38;5;241m=[39m[[38;5;124m"[39m[38;5;124mwords[39m[38;5;124m"[39m])
   :
   : [0;31mNameError[0m: name 'wikipedia_df' is not defined
   :END:


Now lets combine them and remove any duplicates.
#+begin_src jupyter-python
combined_df = pd.concat([])
#+end_src
